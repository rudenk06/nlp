# -*- coding: utf-8 -*-
"""Untitled9.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1xY2M20RFKBAGLxXnoQEPHugSfWtI0Esk
"""

!pip install -U datasets

import pandas as pd
import numpy as np
from datasets import load_dataset

# 1.1. Загрузка AG News
ag = load_dataset("ag_news")
texts = ag["train"]["text"] + ag["test"]["text"]
labels = np.concatenate([ag["train"]["label"], ag["test"]["label"]])

# 1.2. Построение DataFrame
df = pd.DataFrame({
    'text': texts,
    'label': labels
})

# 1.3. Проверка на пропуски и базовая статистика
print("Nulls per column:")
print(df.isnull().sum())
print("\nClass distribution:")
print(df['label'].value_counts().sort_index())

import matplotlib.pyplot as plt
import seaborn as sns

# 2.1. Создаем «ручные» признаки
df['char_count'] = df['text'].apply(len)
df['word_count'] = df['text'].apply(lambda x: len(x.split()))
df['avg_word_len'] = df['char_count'] / df['word_count']
df['punct_count'] = df['text'].apply(lambda x: sum(c in '.,;:!?' for c in x))
stopwords = {'the','and','is','to','in','of','for','on','with'}
df['stopword_count'] = df['text'].apply(lambda x: sum(w in stopwords for w in x.lower().split()))

# 2.2. Корреляционная матрица
features = ['char_count','word_count','avg_word_len','punct_count','stopword_count']
corr = df[features].corr()

plt.figure(figsize=(6,5))
sns.heatmap(corr, annot=True, fmt=".2f", cmap='coolwarm')
plt.title('Correlation Matrix of Text Features')
plt.show()

from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from sklearn.model_selection import train_test_split

# 3.1. Токенизация и паддинг
maxlen = 200
tokenizer = Tokenizer()
tokenizer.fit_on_texts(df['text'])
sequences = tokenizer.texts_to_sequences(df['text'])
X = pad_sequences(sequences, maxlen=maxlen)
y = df['label'].values

# 3.2. Разбиение на train/val/test
X_trval, X_test, y_trval, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)
X_train, X_val, y_train, y_val = train_test_split(
    X_trval, y_trval, test_size=0.1, random_state=42, stratify=y_trval
)

vocab_size = len(tokenizer.word_index) + 1

from tensorflow import keras
from keras import regularizers
from tensorflow.keras.models import Model
from tensorflow.keras.layers import (
    Input, Embedding, SpatialDropout1D,
    Bidirectional, LSTM, Attention,
    GlobalAveragePooling1D, Dense
)
from tensorflow.keras.optimizers import Adam

def build_and_train_model():
    inp = Input(shape=(maxlen,))

    # 4.1. Embedding
    x = Embedding(
        input_dim=vocab_size,
        output_dim=100,       # embed_dim=100: компромисс качество/скорость
        input_length=maxlen
    )(inp)

    # 4.2. SpatialDropout1D
    x = SpatialDropout1D(0.3)(x)

    # 4.3. Bidirectional LSTM
    x = Bidirectional(LSTM(
        64,                   # lstm_units=64: емкость без избыточности
        dropout=0.3,
        kernel_regularizer=regularizers.l2(0.001),
        recurrent_regularizer=regularizers.l2(0.001),
        return_sequences=True  # для Attention
    ))(x)

    # 4.4. Attention + Pooling
    attn = Attention()([x, x])
    x = GlobalAveragePooling1D()(attn)

    # 4.5. Выходной слой
    out = Dense(
        units=4,
        activation='softmax',
        kernel_regularizer=regularizers.l2(0.001)
    )(x)

    model = Model(inputs=inp, outputs=out)
    model.compile(
        loss='sparse_categorical_crossentropy',  # метки целыми 0–3
        optimizer=Adam(learning_rate=0.005),
        metrics=['accuracy']
    )

    # 4.6. Обучение с EarlyStopping
    history = model.fit(
        X_train, y_train,
        validation_data=(X_val, y_val),
        epochs=5,
        batch_size=64,
        callbacks=[keras.callbacks.EarlyStopping(
            monitor='val_accuracy',
            patience=3,
            restore_best_weights=True
        )]
    )
    return model, history

model, history = build_and_train_model()

import matplotlib.pyplot as plt

plt.figure(figsize=(12,5))

# 5.1. Loss
plt.subplot(1,2,1)
plt.plot(history.history['loss'], label='train_loss')
plt.plot(history.history['val_loss'], label='val_loss')
plt.title('Loss over Epochs')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()

# 5.2. Accuracy
plt.subplot(1,2,2)
plt.plot(history.history['accuracy'], label='train_acc')
plt.plot(history.history['val_accuracy'], label='val_acc')
plt.title('Accuracy over Epochs')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()

plt.tight_layout()
plt.show()

from sklearn.metrics import f1_score

y_pred = model.predict(X_test, batch_size=64)
y_pred = np.argmax(y_pred, axis=1)

f1 = f1_score(y_test, y_pred, average='weighted')
print(f"Test F1-score: {f1:.4f}")